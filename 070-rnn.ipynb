{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "![LSTM](imgs/LSTM3-chain.png)\n",
    "\n",
    "![LSTM](imgs/LSTM2-notation.png)\n",
    "\n",
    "\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-f.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-i.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-C.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pytorch documentation\n",
    "\n",
    "\\begin{array}{ll} \\\\\n",
    "    f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "    i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "    g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "    c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "    o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\    \n",
    "    h_t = o_t * \\tanh(c_t) \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8835, -0.0504,  0.8493],\n",
       "         [ 0.9167, -1.7890, -0.0660]],\n",
       "\n",
       "        [[-1.3275, -2.4630, -0.1197],\n",
       "         [-0.8243, -1.2423, -0.8760]],\n",
       "\n",
       "        [[ 1.4885,  0.6106,  1.1932],\n",
       "         [-1.1535,  0.9402,  1.0332]],\n",
       "\n",
       "        [[ 0.5672, -0.8871,  0.5998],\n",
       "         [ 1.1069, -1.5476, -0.7041]],\n",
       "\n",
       "        [[ 1.0619,  0.9808, -0.3712],\n",
       "         [-0.5278, -0.4647, -0.9003]],\n",
       "\n",
       "        [[ 0.8746,  0.8302,  1.2621],\n",
       "         [-0.6655, -0.3673, -1.1719]],\n",
       "\n",
       "        [[ 0.6300,  1.9727,  0.4929],\n",
       "         [ 0.1673,  0.0748, -0.8296]],\n",
       "\n",
       "        [[-0.1490,  1.3435, -0.3468],\n",
       "         [ 0.6468,  0.2840, -2.7029]],\n",
       "\n",
       "        [[ 0.4469, -0.4357,  0.4136],\n",
       "         [ 2.3348, -0.6733, -0.7259]],\n",
       "\n",
       "        [[ 1.5137,  0.0275,  0.6132],\n",
       "         [ 0.3686,  0.1394,  0.9901]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_size = 3\n",
    "hidden_size = 4 \n",
    "\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hidden_0 = (h_0, c_0)\n",
    "hidden_0 = (torch.zeros(1, batch_size, hidden_size), torch.zeros(1, batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm(inputs)\n",
    "lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0640,  0.0464,  0.1051,  0.0191],\n",
       "         [ 0.2720,  0.0936,  0.0765, -0.0414]],\n",
       "\n",
       "        [[ 0.1944,  0.1321,  0.0993, -0.0421],\n",
       "         [ 0.1642,  0.0201,  0.0750, -0.0731]],\n",
       "\n",
       "        [[ 0.2065,  0.1581,  0.0218,  0.1140],\n",
       "         [ 0.0220, -0.0575,  0.1398, -0.0065]],\n",
       "\n",
       "        [[ 0.2827,  0.2223,  0.0993,  0.0716],\n",
       "         [ 0.2915,  0.0311,  0.0617, -0.0182]],\n",
       "\n",
       "        [[ 0.0796,  0.0630, -0.0630,  0.2852],\n",
       "         [ 0.0971, -0.0696,  0.0629, -0.0192]],\n",
       "\n",
       "        [[ 0.1165,  0.1402,  0.0299,  0.2328],\n",
       "         [ 0.0239, -0.1590,  0.0522, -0.0334]],\n",
       "\n",
       "        [[ 0.0026,  0.0230, -0.0890,  0.2620],\n",
       "         [ 0.0248, -0.1396,  0.0399,  0.0643]],\n",
       "\n",
       "        [[-0.0547, -0.0997, -0.0248,  0.2094],\n",
       "         [-0.1517, -0.1526, -0.0122,  0.1687]],\n",
       "\n",
       "        [[ 0.1376,  0.0074,  0.0915,  0.1350],\n",
       "         [ 0.2518, -0.0673,  0.0108,  0.2985]],\n",
       "\n",
       "        [[ 0.2623,  0.0973,  0.0545,  0.2693],\n",
       "         [ 0.1425,  0.0413,  0.1222,  0.1845]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to put hidden inputs to zeros, there is no need to provide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0640,  0.0464,  0.1051,  0.0191],\n",
       "         [ 0.2720,  0.0936,  0.0765, -0.0414]],\n",
       "\n",
       "        [[ 0.1944,  0.1321,  0.0993, -0.0421],\n",
       "         [ 0.1642,  0.0201,  0.0750, -0.0731]],\n",
       "\n",
       "        [[ 0.2065,  0.1581,  0.0218,  0.1140],\n",
       "         [ 0.0220, -0.0575,  0.1398, -0.0065]],\n",
       "\n",
       "        [[ 0.2827,  0.2223,  0.0993,  0.0716],\n",
       "         [ 0.2915,  0.0311,  0.0617, -0.0182]],\n",
       "\n",
       "        [[ 0.0796,  0.0630, -0.0630,  0.2852],\n",
       "         [ 0.0971, -0.0696,  0.0629, -0.0192]],\n",
       "\n",
       "        [[ 0.1165,  0.1402,  0.0299,  0.2328],\n",
       "         [ 0.0239, -0.1590,  0.0522, -0.0334]],\n",
       "\n",
       "        [[ 0.0026,  0.0230, -0.0890,  0.2620],\n",
       "         [ 0.0248, -0.1396,  0.0399,  0.0643]],\n",
       "\n",
       "        [[-0.0547, -0.0997, -0.0248,  0.2094],\n",
       "         [-0.1517, -0.1526, -0.0122,  0.1687]],\n",
       "\n",
       "        [[ 0.1376,  0.0074,  0.0915,  0.1350],\n",
       "         [ 0.2518, -0.0673,  0.0108,  0.2985]],\n",
       "\n",
       "        [[ 0.2623,  0.0973,  0.0545,  0.2693],\n",
       "         [ 0.1425,  0.0413,  0.1222,  0.1845]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm(inputs)\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the last output is the output of RRR. We can get it by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2623, 0.0973, 0.0545, 0.2693],\n",
       "        [0.1425, 0.0413, 0.1222, 0.1845]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convient to have batches as the first dimension of the input. One can do it by adding `batch_first=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4287, -0.8089,  0.6496],\n",
       "         [ 1.0766, -0.9408,  0.4500],\n",
       "         [-2.0297,  0.4132, -0.3924],\n",
       "         [-0.6662,  1.3396, -0.6382],\n",
       "         [-0.7902, -1.3808, -0.3269],\n",
       "         [ 0.4475,  1.4489,  0.6695],\n",
       "         [ 1.4372, -0.7656,  0.5237],\n",
       "         [ 1.3693, -0.2232,  0.7301],\n",
       "         [ 1.9571, -1.6995,  0.5775],\n",
       "         [-0.0383,  0.9943, -0.8937]],\n",
       "\n",
       "        [[-0.3910,  2.0286, -0.5208],\n",
       "         [ 0.9066, -1.7970,  0.0912],\n",
       "         [-0.4176, -0.2242, -0.8004],\n",
       "         [ 0.3201, -0.6448,  0.0149],\n",
       "         [ 1.4970,  0.3302,  0.4920],\n",
       "         [ 0.3469,  1.1633,  0.5200],\n",
       "         [ 0.4284, -1.3784,  0.3542],\n",
       "         [-0.8269,  0.3485, -0.8807],\n",
       "         [ 0.7595, -0.8450,  0.4487],\n",
       "         [ 0.1381, -1.2362, -1.0107]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_batch_first = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True) \n",
    "inputs_batch_first = torch.randn(batch_size, seq_len, input_size)\n",
    "inputs_batch_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1009,  0.0273,  0.0387, -0.0461],\n",
       "         [ 0.1351,  0.0650,  0.0433, -0.1063],\n",
       "         [ 0.1947, -0.1088,  0.0679, -0.0051],\n",
       "         [ 0.0817, -0.0986,  0.1297,  0.1314],\n",
       "         [ 0.1716, -0.2346,  0.1086,  0.0577],\n",
       "         [ 0.0385, -0.0090,  0.1567,  0.0923],\n",
       "         [ 0.0777,  0.0757,  0.1092, -0.0861],\n",
       "         [ 0.0860,  0.1483,  0.0588, -0.1190],\n",
       "         [ 0.1517,  0.2252, -0.0215, -0.1544],\n",
       "         [ 0.1559,  0.0661,  0.0597,  0.0148]],\n",
       "\n",
       "        [[ 0.0413,  0.0129,  0.0749,  0.1803],\n",
       "         [ 0.1303, -0.0385,  0.0931, -0.0588],\n",
       "         [ 0.1456, -0.1233,  0.1383,  0.0170],\n",
       "         [ 0.1230, -0.1054,  0.1387, -0.0338],\n",
       "         [ 0.0651,  0.0693,  0.1362, -0.0918],\n",
       "         [ 0.0652,  0.0854,  0.1089,  0.0176],\n",
       "         [ 0.1615,  0.0134,  0.0784, -0.0625],\n",
       "         [ 0.1523, -0.1066,  0.1237,  0.0600],\n",
       "         [ 0.1207, -0.0580,  0.0993, -0.0582],\n",
       "         [ 0.1892, -0.1685,  0.1514, -0.0690]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm_batch_first(inputs_batch_first)\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the finial output by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1559,  0.0661,  0.0597,  0.0148],\n",
       "        [ 0.1892, -0.1685,  0.1514, -0.0690]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 19, 19, 26, 91, 40, 57, 78, 89, 31],\n",
       "        [46,  5, 71, 60, 56, 35, 97, 78, 19, 78]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = 100\n",
    "sentences = torch.randint(dict_size, (batch_size, seq_len))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3\n",
    "embedding = nn.Embedding(dict_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1624,  1.3295, -3.2450],\n",
       "         [ 0.1561, -0.2889, -0.1083],\n",
       "         [ 0.1561, -0.2889, -0.1083],\n",
       "         [-1.4842,  0.3282,  0.0872],\n",
       "         [ 1.2635,  2.7697, -0.3629],\n",
       "         [-0.6307,  0.6052,  0.2908],\n",
       "         [ 0.5322, -0.7821, -0.0670],\n",
       "         [ 0.5202, -0.4762, -0.6227],\n",
       "         [-0.4210,  0.3746,  0.2844],\n",
       "         [-0.5235, -1.5446,  1.5872]],\n",
       "\n",
       "        [[ 1.1131,  0.4510,  0.8111],\n",
       "         [-0.8521, -0.2145,  0.1316],\n",
       "         [ 0.3181, -0.9739,  0.3885],\n",
       "         [-1.0910, -2.0403, -0.7762],\n",
       "         [ 0.1402, -0.5982, -0.6415],\n",
       "         [-0.8607, -1.0654, -1.3223],\n",
       "         [-0.4741, -0.8455, -0.5330],\n",
       "         [ 0.5202, -0.4762, -0.6227],\n",
       "         [ 0.1561, -0.2889, -0.1083],\n",
       "         [ 0.5202, -0.4762, -0.6227]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_embedded = embedding(sentences)\n",
    "sentences_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1401, -0.0974,  0.0119, -0.0081],\n",
       "        [ 0.0828, -0.0992,  0.2193, -0.0599]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, _ = lstm_batch_first(sentences_embedded)\n",
    "lstm_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento del lenguaje natural\n",
    "\n",
    "Tenemos un dataset que tiene textos que queremos evaluar si son tóxicos o no tóxicos. Este dataset puedes bajarlo del siguiente enlace \n",
    "[aquí](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments_df = pd.read_csv(\"data/jigsaw-toxic-comment-classification-challenge/train.csv\")[:1000]\n",
    "comments_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>\"\\n\\nNo not really. We may ask that the mentio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>\"\\nHaha, you're fine. I mean, you're allowed t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text\n",
       "744  \"\\n\\nNo not really. We may ask that the mentio...\n",
       "981  \"\\nHaha, you're fine. I mean, you're allowed t..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label_colnames = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(comments_df[['comment_text']], comments_df[label_colnames], random_state=667)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
    "STEMMER = SnowballStemmer('english')\n",
    "\n",
    "class TextPreprocessor:\n",
    "        \n",
    "    def transfrom_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(BAD_SYMBOLS_RE, \" \", text) # process bad symbols\n",
    "        # text = \" \".join([STEMMER.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.apply(lambda text: self.transfrom_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "X_train_preprocessed = preprocessor.transform(X_train['comment_text'])\n",
    "X_test_preprocessed = preprocessor.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "No not really. We may ask that the mention of fat being the fire source of the cremation of millions be reconsidered though - along with a few other items. The fat cremation \"\"wiki fact\"\" is citable ( www.hdot - Emory U no less, Lipstadt) but doubtful. If the same science was applied to the holocaust as say the tinfoilers or flat earthers the deniers would be overjoyed. Be careful as to who gets the nutty fringe tinfoil label in the end. You get the permits and we'll bring the shovels. 159.105.80.141  \"\n",
      "   no not really  we may ask that the mention of fat being the fire source of the cremation of millions be reconsidered though   along with a few other items  the fat cremation   wiki fact   is citable   www hdot   emory u no less  lipstadt  but doubtful  if the same science was applied to the holocaust as say the tinfoilers or flat earthers the deniers would be overjoyed  be careful as to who gets the nutty fringe tinfoil label in the end  you get the permits and we ll bring the shovels  159 105 80 141   \n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[0])\n",
    "print(X_train_preprocessed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Haha, you're fine. I mean, you're allowed to do it, but I'm just selfish, I guess. =) I really appreciate your kindness, though. And I really respect that you asked, because when other signatures that were borrowed, no one let me know or gave me any credit! So I feel badly that since you asked, you'd feel really badly about doing it now, haha. But I can help you figure out a nice one or pick out some fun colors. Have a great day, and happy Wikying! τ \"\n",
      "  haha  you re fine  i mean  you re allowed to do it  but i m just selfish  i guess     i really appreciate your kindness  though  and i really respect that you asked  because when other signatures that were borrowed  no one let me know or gave me any credit  so i feel badly that since you asked  you d feel really badly about doing it now  haha  but i can help you figure out a nice one or pick out some fun colors  have a great day  and happy wikying     \n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[1])\n",
    "print(X_train_preprocessed.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(text):\n",
    "    word_set = set()\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "    word_list = [\"<UNK>\", \"<PAD>\"] + sorted(list(word_set))\n",
    "    word2idx = {word_list[idx]: idx for idx in range(len(word_list))}\n",
    "    idx2word = {idx: word_list[idx] for idx in range(len(word_list))}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = None\n",
    "        self.idx2word = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        text = \" \".join(X)\n",
    "        self.word2idx, self.idx2word = create_dicts(text)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.transform_line(line) for line in X]\n",
    "        \n",
    "    def transform_line(self, line):\n",
    "        return [self.word2idx.get(word, 0) for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer.transform(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutter:\n",
    "\n",
    "    def __init__(self, size=150):\n",
    "        self.size = size\n",
    "        \n",
    "    def transform(self, X):\n",
    "        new_X = []\n",
    "        for line in X:\n",
    "            new_line = line[:self.size]\n",
    "            new_line = new_line + [1] * (self.size - len(new_line))\n",
    "            new_X.append(new_line)\n",
    "        return new_X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Cutter()\n",
    "X_train_cutted = cutter.transform(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.from_numpy(y_train.values)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.tensor(X_train_cutted), torch.from_numpy(y_train.values).float())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dict_size, output_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(dict_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeded)\n",
    "        lstm_out = lstm_out[:, -1]        \n",
    "        logits = self.fc(lstm_out)\n",
    "        out = self.sigmoid(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(tokenizer.word2idx)\n",
    "output_size = len(label_colnames)\n",
    "embedding_dim = 3\n",
    "hidden_dim = 4\n",
    "\n",
    "lstm_model = LSTMModel(dict_size, output_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750, 150])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch = torch.tensor(X_train_cutted)\n",
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4708, 0.4680, 0.4677, 0.4272, 0.5332, 0.5480],\n",
       "        ...,\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4881, 0.4844, 0.4511, 0.4564, 0.5862, 0.5416]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model(X_train_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4743, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4789, 0.4698, 0.4569, 0.4406, 0.5512, 0.5545],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4762, 0.4703, 0.4613, 0.4401, 0.5520, 0.5469],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572],\n",
       "        [0.4744, 0.4811, 0.4592, 0.4354, 0.5314, 0.5572]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for batch_i, (input_data, labels) in enumerate(train_loader):\n",
    "        # Zero gradients (just in case)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, calculate predictions\n",
    "        output = lstm_model(input_data) \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        ## Backward propagation\n",
    "        loss.backward()\n",
    "        ## Upade weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
